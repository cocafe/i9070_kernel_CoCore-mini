From a7775870dfd7faa5b1cb8892b28b4c95c9fc87bb Mon Sep 17 00:00:00 2001
From: HomuHomu <ecoco666@gmail.com>
Date: Tue, 7 Aug 2012 22:00:19 +0900
Subject: [PATCH] ZRAM for ANDROID

---
 drivers/staging/android/lowmemorykiller.c |  332 +++++++++++++++++++++++++++++
 drivers/staging/zram/Kconfig              |    7 +
 drivers/staging/zram/zram_drv.c           |   53 ++++-
 drivers/staging/zram/zram_drv.h           |    2 +-
 drivers/staging/zram/zram_sysfs.c         |   40 +++-
 kernel/power/earlysuspend.c               |   14 ++
 mm/shmem.c                                |    9 +
 mm/swapfile.c                             |  153 +++++++++++++
 mm/vmscan.c                               |   91 +++++++-
 9 files changed, 688 insertions(+), 13 deletions(-)

diff --git a/drivers/staging/android/lowmemorykiller.c b/drivers/staging/android/lowmemorykiller.c
index 0d5d907..f0864b1 100644
--- a/drivers/staging/android/lowmemorykiller.c
+++ b/drivers/staging/android/lowmemorykiller.c
@@ -36,6 +36,12 @@
 #include <linux/sched.h>
 #include <linux/rcupdate.h>
 #include <linux/notifier.h>
+#ifdef CONFIG_ZRAM_FOR_ANDROID
+#include <linux/swap.h>
+#include <linux/device.h>
+#include <linux/err.h>
+#include <linux/mm_inline.h>
+#endif /* CONFIG_ZRAM_FOR_ANDROID */
 #include <linux/memory.h>
 #include <linux/memory_hotplug.h>
 
@@ -54,6 +60,82 @@
 	16 * 1024,	/* 64MB */
 };
 static int lowmem_minfree_size = 4;
+#ifdef CONFIG_ZRAM_FOR_ANDROID
+static struct class *lmk_class;
+static struct device *lmk_dev;
+static int lmk_kill_pid = 0;
+static int lmk_kill_ok = 0;
+#define MM_SWAPON 0xaa
+
+extern atomic_t optimize_comp_on;
+
+extern int isolate_lru_page(struct page *page);
+extern void putback_lru_page(struct page *page);
+extern unsigned int zone_id_shrink_pagelist(struct zone *zone_id,struct list_head *page_list);
+
+#define lru_to_page(_head) (list_entry((_head)->prev, struct page, lru))
+
+#define SWAP_PROCESS_DEBUG_LOG 1
+/* free RAM 8M(6250 pages) */
+#define CHECK_FREE_MEMORY 2048
+/* free swap (10240 pages) */
+#define CHECK_FREE_SWAPSPACE  10240
+
+struct scan_control {
+	/* Incremented by the number of inactive pages that were scanned */
+	unsigned long nr_scanned;
+
+	/* Number of pages freed so far during a call to shrink_zones() */
+	unsigned long nr_reclaimed;
+
+	/* How many pages shrink_list() should reclaim */
+	unsigned long nr_to_reclaim;
+
+	unsigned long hibernation_mode;
+
+	/* This context's GFP mask */
+	gfp_t gfp_mask;
+
+	int may_writepage;
+
+	/* Can mapped pages be reclaimed? */
+	int may_unmap;
+
+	/* Can pages be swapped as part of reclaim? */
+	int may_swap;
+
+	int swappiness;
+
+	int order;
+
+	/*
+	 * Intend to reclaim enough contenious memory rather than to reclaim
+	 * enough amount memory. I.e, it's the mode for high order allocation.
+	 */
+	bool lumpy_reclaim_mode;
+
+	/* Which cgroup do we reclaim from */
+	struct mem_cgroup *mem_cgroup;
+
+	/*
+	 * Nodemask of nodes allowed by the caller. If NULL, all nodes
+	 * are scanned.
+	 */
+	nodemask_t	*nodemask;
+};
+
+enum pageout_io {
+	PAGEOUT_IO_ASYNC,
+	PAGEOUT_IO_SYNC,
+};
+
+extern unsigned long shrink_page_list(struct list_head *page_list,
+				      struct zone *zone,
+				      struct scan_control *sc);
+extern unsigned long clear_active_flags(struct list_head *page_list,
+					unsigned int *count);
+
+#endif /* CONFIG_ZRAM_FOR_ANDROID */
 
 static unsigned int offlining;
 static struct task_struct *lowmem_deathpending;
@@ -235,6 +317,256 @@ static int lowmem_shrink(struct shrinker *s, struct shrink_control *sc)
 	.seeks = DEFAULT_SEEKS * 16
 };
 
+#ifdef CONFIG_ZRAM_FOR_ANDROID
+/*
+ * zone_id_shrink_pagelist() clear page flags,
+ * update the memory zone status, and swap pagelist
+ */
+
+static unsigned int shrink_pages(struct mm_struct *mm, struct zone **zone_id_0,
+				 struct list_head *zone0_page_list,
+				 struct zone **zone_id_1,
+				 struct list_head *zone1_page_list,
+				 unsigned int num_to_scan)
+{
+	unsigned long addr;
+	unsigned int isolate_pages_countter = 0;
+
+	struct vm_area_struct *vma = mm->mmap;
+	while (vma != NULL) {
+
+		for (addr = vma->vm_start; addr < vma->vm_end;
+		     addr += PAGE_SIZE) {
+			struct page *page;
+			/*get the page address from virtual memory address */
+			page = follow_page(vma, addr, FOLL_GET);
+
+			if (page && !IS_ERR(page)) {
+
+				put_page(page);
+				/* only moveable, anonymous and not dirty pages can be swapped  */
+				if ((!PageUnevictable(page))
+				    && (!PageDirty(page)) && ((PageAnon(page)))
+				    && (0 == page_is_file_cache(page))) {
+					switch (page_zone_id(page)) {
+					case 0:
+						if (!isolate_lru_page(page)) {
+							/* isolate page from LRU and add to temp list  */
+							*zone_id_0 =
+							    page_zone(page);
+							/*create new page list, it will be used in shrink_page_list */
+							spin_lock_irq(&(*zone_id_0)->lru_lock);
+							list_add_tail(&page->lru, zone0_page_list);
+							spin_unlock_irq(&(*zone_id_0)->lru_lock);
+							isolate_pages_countter++;
+						}
+						break;
+					case 1:
+						if (!isolate_lru_page(page)) {
+							/* isolate page from LRU and add to temp list  */
+							*zone_id_1 =
+							    page_zone(page);
+							/*create new page list, it will be used in shrink_page_list */
+							spin_lock_irq(&(*zone_id_1)->lru_lock);
+							list_add_tail(&page->lru, zone1_page_list);
+							spin_unlock_irq(&(*zone_id_1)->lru_lock);
+							isolate_pages_countter++;
+						}
+						break;
+					default:
+						break;
+					}
+				}
+			}
+
+			if (isolate_pages_countter >= num_to_scan) {
+				return isolate_pages_countter;
+			}
+		}
+
+		vma = vma->vm_next;
+	}
+
+	return isolate_pages_countter;
+}
+
+/*
+ * swap_application_pages() will search the
+ * pages which can be swapped, then call
+ * zone_id_shrink_pagelist to update zone
+ * status
+ */
+static unsigned int swap_pages(struct zone *zone_id_0,
+			       struct list_head *zone0_page_list,
+			       struct zone *zone_id_1,
+			       struct list_head *zone1_page_list)
+{
+	unsigned int pages_counter = 0;
+
+	/*if the page list is not empty, call zone_id_shrink_pagelist to update zone status */
+	if ((zone_id_0) && (!list_empty(zone0_page_list))) {
+		pages_counter +=
+		    zone_id_shrink_pagelist(zone_id_0, zone0_page_list);
+	}
+	if ((zone_id_1) && (!list_empty(zone1_page_list))) {
+		pages_counter +=
+		    zone_id_shrink_pagelist(zone_id_1, zone1_page_list);
+	}
+	return pages_counter;
+}
+
+static ssize_t lmk_state_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%d,%d\n", lmk_kill_pid, lmk_kill_ok);
+}
+
+/*
+ * lmk_state_store() will called by framework,
+ * the framework will send the pid of process that need to be swapped
+ */
+static ssize_t lmk_state_store(struct device *dev,
+			       struct device_attribute *attr,
+			       const char *buf, size_t size)
+{
+	sscanf(buf, "%d,%d", &lmk_kill_pid, &lmk_kill_ok);
+
+	/* if the screen on, the optimized compcache will stop */
+	if (atomic_read(&optimize_comp_on) != 1)
+		return size;
+
+	if (lmk_kill_ok == 1) {
+		struct task_struct *p;
+		struct task_struct *selected = NULL;
+		struct sysinfo ramzswap_info = { 0 };
+
+		/*
+		 * check the free RAM and swap area,
+		 * stop the optimized compcache in cpu idle case;
+		 * leave some swap area for using in low memory case
+		 */
+		si_swapinfo(&ramzswap_info);
+		si_meminfo(&ramzswap_info);
+
+		if ((ramzswap_info.freeswap < CHECK_FREE_SWAPSPACE) ||
+		    (ramzswap_info.freeram < CHECK_FREE_MEMORY)) {
+			lmk_kill_ok = 0;
+			return size;
+		}
+
+		read_lock(&tasklist_lock);
+		for_each_process(p) {
+			if ((p->pid == lmk_kill_pid) &&
+			    (__task_cred(p)->uid > 10000)) {
+				task_lock(p);
+				selected = p;
+				if (!selected->mm || !selected->signal) {
+					task_unlock(p);
+					selected = NULL;
+					pr_info("idletime compcache: process is being killed\n");
+					break;
+				}
+				else {
+#if SWAP_PROCESS_DEBUG_LOG > 0
+					pr_info("idletime compcache: swap process pid %d, name %s, task_size %ld\n",
+						p->pid, p->comm, get_mm_rss(p->mm));
+#endif
+				}
+				break;
+			}
+		}
+		read_unlock(&tasklist_lock);
+
+		if (selected) {
+			struct zone *zone0 = NULL, *zone1 = NULL;
+			LIST_HEAD(zone0_page_list);
+			LIST_HEAD(zone1_page_list);
+			int pages_tofree = 0, pages_freed = 0;
+
+			pages_tofree =
+			    shrink_pages(selected->mm, &zone0, &zone0_page_list,
+					 &zone1, &zone1_page_list, 0x7FFFFFFF);
+			task_unlock(selected);
+			pages_freed =
+			    swap_pages(zone0, &zone0_page_list, zone1,
+				       &zone1_page_list);
+			lmk_kill_ok = 0;
+
+		}
+	}
+
+	return size;
+}
+
+static DEVICE_ATTR(lmk_state, 0664, lmk_state_show, lmk_state_store);
+
+/*
+ *  swap_inactive_pagelist() will be called in low memory case,
+ *  swap SWAP_CLUSTER_MAX pages to swap space
+ */
+int swap_inactive_pagelist(unsigned int page_swap_cluster)
+{
+	struct task_struct *p, *selected = NULL;
+	int tasksize;
+	int hidden_min_oom_adj = 9;
+	int pages_counter = 0;
+
+	read_lock(&tasklist_lock);
+	for_each_process(p) {
+		struct mm_struct *mm;
+		struct signal_struct *sig;
+		int oom_adj;
+
+		task_lock(p);
+		mm = p->mm;
+		sig = p->signal;
+		if (!mm || !sig) {
+			task_unlock(p);
+			continue;
+		}
+
+		tasksize = get_mm_rss(mm);
+
+		if (tasksize <= 0) {
+			task_unlock(p);
+			continue;
+		}
+
+		oom_adj = sig->oom_adj;
+		if (oom_adj >= hidden_min_oom_adj) {
+			selected = p;
+#if SWAP_PROCESS_DEBUG_LOG > 0
+			printk
+			    ("runtime compcache: swap process pid %d, name %s, oom %d\n",
+			     p->pid, p->comm, oom_adj);
+#endif
+			break;
+		}
+		task_unlock(p);
+	}
+	read_unlock(&tasklist_lock);
+
+	if (selected) {
+		struct zone *zone0 = NULL, *zone1 = NULL;
+		LIST_HEAD(zone0_page_list);
+		LIST_HEAD(zone1_page_list);
+
+		shrink_pages(selected->mm, &zone0, &zone0_page_list, &zone1,
+			     &zone1_page_list, 32);
+		task_unlock(selected);
+		pages_counter =
+		    swap_pages(zone0, &zone0_page_list, zone1,
+			       &zone1_page_list);
+		printk("pagefreed = %d\n", pages_counter);
+	}
+
+	return pages_counter;
+}
+EXPORT_SYMBOL(swap_inactive_pagelist);
+
+
+#endif /* CONFIG_ZRAM_FOR_ANDROID */
+
 static int __init lowmem_init(void)
 {
 	task_free_register(&task_nb);
diff --git a/drivers/staging/zram/Kconfig b/drivers/staging/zram/Kconfig
index 3bec4db..06f741a 100644
--- a/drivers/staging/zram/Kconfig
+++ b/drivers/staging/zram/Kconfig
@@ -28,3 +28,10 @@ config ZRAM_DEBUG
 	help
 	  This option adds additional debugging code to the compressed
 	  RAM block device driver.
+
+config ZRAM_FOR_ANDROID
+	bool "Optimize zram behavior for android"
+	depends on ZRAM && ANDROID
+	default n
+	help
+	  This option enables modified zram behavior optimized for android
diff --git a/drivers/staging/zram/zram_drv.c b/drivers/staging/zram/zram_drv.c
index aab4ec4..5258c78 100644
--- a/drivers/staging/zram/zram_drv.c
+++ b/drivers/staging/zram/zram_drv.c
@@ -32,12 +32,16 @@
 #include <linux/lzo.h>
 #include <linux/string.h>
 #include <linux/vmalloc.h>
+#ifdef CONFIG_ZRAM_FOR_ANDROID
+#include <linux/swap.h>
+#endif /* CONFIG_ZRAM_FOR_ANDROID */
+
 
 #include "zram_drv.h"
 
 /* Globals */
 static int zram_major;
-struct zram *devices;
+struct zram *zram_devices;
 
 /* Module params (documentation at end) */
 unsigned int num_devices;
@@ -133,6 +137,22 @@ static void zram_set_disksize(struct zram *zram, size_t totalram_bytes)
 	zram->disksize &= PAGE_MASK;
 }
 
+#ifdef CONFIG_ZRAM_FOR_ANDROID
+/*
+ * Swap header (1st page of swap device) contains information
+ * about a swap file/partition. Prepare such a header for the
+ * given ramzswap device so that swapon can identify it as a
+ * swap partition.
+ */
+static void setup_swap_header(struct zram *zram, union swap_header *s)
+{
+	s->info.version = 1;
+	s->info.last_page = (zram->disksize >> PAGE_SHIFT) - 1;
+	s->info.nr_badpages = 0;
+	memcpy(s->magic.magic, "SWAPSPACE2", 10);
+}
+#endif /* CONFIG_ZRAM_FOR_ANDROID */
+
 static void zram_free_page(struct zram *zram, size_t index)
 {
 	u32 clen;
@@ -501,6 +521,10 @@ int zram_init_device(struct zram *zram)
 {
 	int ret;
 	size_t num_pages;
+#ifdef CONFIG_ZRAM_FOR_ANDROID
+	struct page *page;
+	union swap_header *swap_header;
+#endif /* CONFIG_ZRAM_FOR_ANDROID */
 
 	mutex_lock(&zram->init_lock);
 
@@ -535,6 +559,19 @@ int zram_init_device(struct zram *zram)
 		goto fail;
 	}
 
+#ifdef CONFIG_ZRAM_FOR_ANDROID
+	page = alloc_page(__GFP_ZERO);
+	if (!page) {
+		pr_err("Error allocating swap header page\n");
+		ret = -ENOMEM;
+		goto fail;
+	}
+	zram->table[0].page = page;
+	zram_set_flag(zram, 0, ZRAM_UNCOMPRESSED);
+	swap_header = kmap(page);
+	setup_swap_header(zram, swap_header);
+	kunmap(page);
+#endif /* CONFIG_ZRAM_FOR_ANDROID */
 	set_capacity(zram->disk, zram->disksize >> SECTOR_SHIFT);
 
 	/* zram devices sort of resembles non-rotational disks */
@@ -678,14 +715,14 @@ static int __init zram_init(void)
 
 	/* Allocate the device array and initialize each one */
 	pr_info("Creating %u devices ...\n", num_devices);
-	devices = kzalloc(num_devices * sizeof(struct zram), GFP_KERNEL);
-	if (!devices) {
+	zram_devices = kzalloc(num_devices * sizeof(struct zram), GFP_KERNEL);
+	if (!zram_devices) {
 		ret = -ENOMEM;
 		goto unregister;
 	}
 
 	for (dev_id = 0; dev_id < num_devices; dev_id++) {
-		ret = create_device(&devices[dev_id], dev_id);
+		ret = create_device(&zram_devices[dev_id], dev_id);
 		if (ret)
 			goto free_devices;
 	}
@@ -694,8 +731,8 @@ static int __init zram_init(void)
 
 free_devices:
 	while (dev_id)
-		destroy_device(&devices[--dev_id]);
-	kfree(devices);
+		destroy_device(&zram_devices[--dev_id]);
+	kfree(zram_devices);
 unregister:
 	unregister_blkdev(zram_major, "zram");
 out:
@@ -708,7 +745,7 @@ static void __exit zram_exit(void)
 	struct zram *zram;
 
 	for (i = 0; i < num_devices; i++) {
-		zram = &devices[i];
+		zram = &zram_devices[i];
 
 		destroy_device(zram);
 		if (zram->init_done)
@@ -717,7 +754,7 @@ static void __exit zram_exit(void)
 
 	unregister_blkdev(zram_major, "zram");
 
-	kfree(devices);
+	kfree(zram_devices);
 	pr_debug("Cleanup done!\n");
 }
 
diff --git a/drivers/staging/zram/zram_drv.h b/drivers/staging/zram/zram_drv.h
index 408b2c0..3ad9486 100644
--- a/drivers/staging/zram/zram_drv.h
+++ b/drivers/staging/zram/zram_drv.h
@@ -120,7 +120,7 @@ struct zram {
 	struct zram_stats stats;
 };
 
-extern struct zram *devices;
+extern struct zram *zram_devices;
 extern unsigned int num_devices;
 #ifdef CONFIG_SYSFS
 extern struct attribute_group zram_disk_attr_group;
diff --git a/drivers/staging/zram/zram_sysfs.c b/drivers/staging/zram/zram_sysfs.c
index a70cc01..8a23554 100644
--- a/drivers/staging/zram/zram_sysfs.c
+++ b/drivers/staging/zram/zram_sysfs.c
@@ -35,7 +35,7 @@ static struct zram *dev_to_zram(struct device *dev)
 	struct zram *zram = NULL;
 
 	for (i = 0; i < num_devices; i++) {
-		zram = &devices[i];
+		zram = &zram_devices[i];
 		if (disk_to_dev(zram->disk) == dev)
 			break;
 	}
@@ -80,6 +80,42 @@ static ssize_t initstate_show(struct device *dev,
 	return sprintf(buf, "%u\n", zram->init_done);
 }
 
+#ifdef CONFIG_ZRAM_FOR_ANDROID
+extern int swapon(const char*specialfile, int swap_flags);
+
+static ssize_t initstate_store(struct device *dev,
+			       struct device_attribute *attr, const char *buf,
+			       size_t len)
+{
+	int ret;
+	unsigned long do_init;
+	struct zram *zram = dev_to_zram(dev);
+
+	if (zram->init_done) {
+		pr_info("the device is initialized device\n");
+		return -EBUSY;
+	}
+
+	ret = strict_strtoul(buf, 10, &do_init);
+	if (ret)
+		return ret;
+	if (!do_init)
+		return -EINVAL;
+
+	zram_init_device(zram);
+	swapon("/dev/block/zram0", 0);
+	return len;
+}
+#else
+static inline ssize_t initstate_store(struct device *dev,
+				      struct device_attribute *attr,
+				      const char *buf, size_t len)
+{
+	return 0;
+}
+#endif /* CONFIG_ZRAM_FOR_ANDROID */
+
+
 static ssize_t reset_store(struct device *dev,
 		struct device_attribute *attr, const char *buf, size_t len)
 {
@@ -190,7 +226,7 @@ static ssize_t mem_used_total_show(struct device *dev,
 
 static DEVICE_ATTR(disksize, S_IRUGO | S_IWUSR,
 		disksize_show, disksize_store);
-static DEVICE_ATTR(initstate, S_IRUGO, initstate_show, NULL);
+static DEVICE_ATTR(initstate, S_IRUGO | S_IWUSR, initstate_show, initstate_store);
 static DEVICE_ATTR(reset, S_IWUSR, NULL, reset_store);
 static DEVICE_ATTR(num_reads, S_IRUGO, num_reads_show, NULL);
 static DEVICE_ATTR(num_writes, S_IRUGO, num_writes_show, NULL);
diff --git a/kernel/power/earlysuspend.c b/kernel/power/earlysuspend.c
index 450c3ca..dafeb92 100644
--- a/kernel/power/earlysuspend.c
+++ b/kernel/power/earlysuspend.c
@@ -19,6 +19,9 @@
 #include <linux/rtc.h>
 #include <linux/wakelock.h>
 #include <linux/workqueue.h>
+#ifdef CONFIG_ZRAM_FOR_ANDROID
+#include <asm/atomic.h>
+#endif /* CONFIG_ZRAM_FOR_ANDROID */
 
 #include "power.h"
 
@@ -28,6 +31,11 @@ enum {
 	DEBUG_VERBOSE = 1U << 3,
 };
 static int debug_mask = DEBUG_USER_STATE | DEBUG_SUSPEND;
+#ifdef CONFIG_ZRAM_FOR_ANDROID
+atomic_t optimize_comp_on = ATOMIC_INIT(0);
+EXPORT_SYMBOL(optimize_comp_on);
+#endif /* CONFIG_ZRAM_FOR_ANDROID */
+
 module_param_named(debug_mask, debug_mask, int, S_IRUGO | S_IWUSR | S_IWGRP);
 
 static DEFINE_MUTEX(early_suspend_lock);
@@ -78,6 +86,9 @@ static void early_suspend(struct work_struct *work)
 
 	mutex_lock(&early_suspend_lock);
 	spin_lock_irqsave(&state_lock, irqflags);
+#ifdef CONFIG_ZRAM_FOR_ANDROID
+	atomic_set(&optimize_comp_on, 1);
+#endif /* CONFIG_ZRAM_FOR_ANDROID */
 	if (state == SUSPEND_REQUESTED)
 		state |= SUSPENDED;
 	else
@@ -123,6 +134,9 @@ static void late_resume(struct work_struct *work)
 
 	mutex_lock(&early_suspend_lock);
 	spin_lock_irqsave(&state_lock, irqflags);
+#ifdef CONFIG_ZRAM_FOR_ANDROID
+	atomic_set(&optimize_comp_on, 0);
+#endif /* CONFIG_ZRAM_FOR_ANDROID */
 	if (state == SUSPENDED)
 		state &= ~SUSPENDED;
 	else
diff --git a/mm/shmem.c b/mm/shmem.c
index 883e98f..e76f74e 100644
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@ -1060,8 +1060,17 @@ static int shmem_writepage(struct page *page, struct writeback_control *wbc)
 	info = SHMEM_I(inode);
 	if (info->flags & VM_LOCKED)
 		goto redirty;
+#ifdef CONFIG_ZRAM_FOR_ANDROID
+	/*
+	 * Modification for compcache
+	 * shmem_writepage can be reason of kernel panic when using swap.
+	 * This modification prevent using swap by shmem.
+	 */
+	goto redirty;
+#else
 	if (!total_swap_pages)
 		goto redirty;
+#endif
 
 	/*
 	 * shmem_backing_dev_info's capabilities prevent regular writeback or
diff --git a/mm/swapfile.c b/mm/swapfile.c
index c8f4338..3e5a3a7 100644
--- a/mm/swapfile.c
+++ b/mm/swapfile.c
@@ -2012,6 +2012,159 @@ static int setup_swap_map_and_extents(struct swap_info_struct *p,
 	return nr_extents;
 }
 
+#ifdef CONFIG_ZRAM_FOR_ANDROID
+int swapon(char *name, int swap_flags)
+{
+	struct swap_info_struct *p;
+
+	struct file *swap_file = NULL;
+	struct address_space *mapping;
+	int i;
+	int prio;
+	int error;
+	union swap_header *swap_header;
+	int nr_extents;
+	sector_t span;
+	unsigned long maxpages;
+	unsigned char *swap_map = NULL;
+	struct page *page = NULL;
+	struct inode *inode = NULL;
+
+	p = alloc_swap_info();
+	if (IS_ERR(p))
+		return PTR_ERR(p);
+
+	swap_file = filp_open(name, O_RDWR | O_LARGEFILE, 0);
+	if (IS_ERR(swap_file)) {
+		error = PTR_ERR(swap_file);
+		swap_file = NULL;
+		printk("zfqin, filp_open failed\n");
+		goto bad_swap;
+	}
+
+	printk("zfqin, filp_open succeeded\n");
+	p->swap_file = swap_file;
+	mapping = swap_file->f_mapping;
+
+	for (i = 0; i < nr_swapfiles; i++) {
+		struct swap_info_struct *q = swap_info[i];
+
+		if (q == p || !q->swap_file)
+			continue;
+		if (mapping == q->swap_file->f_mapping) {
+			error = -EBUSY;
+			goto bad_swap;
+		}
+	}
+
+	inode = mapping->host;
+	/* If S_ISREG(inode->i_mode) will do mutex_lock(&inode->i_mutex); */
+	error = claim_swapfile(p, inode);
+	if (unlikely(error))
+		goto bad_swap;
+
+	/*
+	 * Read the swap header.
+	 */
+	if (!mapping->a_ops->readpage) {
+		error = -EINVAL;
+		goto bad_swap;
+	}
+	page = read_mapping_page(mapping, 0, swap_file);
+	if (IS_ERR(page)) {
+		error = PTR_ERR(page);
+		goto bad_swap;
+	}
+	swap_header = kmap(page);
+
+	maxpages = read_swap_header(p, swap_header, inode);
+	if (unlikely(!maxpages)) {
+		error = -EINVAL;
+		goto bad_swap;
+	}
+
+	/* OK, set up the swap map and apply the bad block list */
+	swap_map = vzalloc(maxpages);
+	if (!swap_map) {
+		error = -ENOMEM;
+		goto bad_swap;
+	}
+
+	error = swap_cgroup_swapon(p->type, maxpages);
+	if (error)
+		goto bad_swap;
+
+	nr_extents = setup_swap_map_and_extents(p, swap_header, swap_map,
+						maxpages, &span);
+	if (unlikely(nr_extents < 0)) {
+		error = nr_extents;
+		goto bad_swap;
+	}
+
+	if (p->bdev) {
+		if (blk_queue_nonrot(bdev_get_queue(p->bdev))) {
+			p->flags |= SWP_SOLIDSTATE;
+			p->cluster_next = 1 + (random32() % p->highest_bit);
+		}
+		if (discard_swap(p) == 0 && (swap_flags & SWAP_FLAG_DISCARD))
+			p->flags |= SWP_DISCARDABLE;
+	}
+
+	mutex_lock(&swapon_mutex);
+	prio = -1;
+	if (swap_flags & SWAP_FLAG_PREFER)
+		prio =
+		    (swap_flags & SWAP_FLAG_PRIO_MASK) >> SWAP_FLAG_PRIO_SHIFT;
+	enable_swap_info(p, prio, swap_map);
+
+	printk(KERN_INFO "Adding %uk swap on %s.  "
+	       "Priority:%d extents:%d across:%lluk %s%s\n",
+	       p->pages << (PAGE_SHIFT - 10), name, p->prio,
+	       nr_extents, (unsigned long long)span << (PAGE_SHIFT - 10),
+	       (p->flags & SWP_SOLIDSTATE) ? "SS" : "",
+	       (p->flags & SWP_DISCARDABLE) ? "D" : "");
+
+	mutex_unlock(&swapon_mutex);
+	atomic_inc(&proc_poll_event);
+	wake_up_interruptible(&proc_poll_wait);
+
+	if (S_ISREG(inode->i_mode))
+		inode->i_flags |= S_SWAPFILE;
+	error = 0;
+	goto out;
+ bad_swap:
+	if (inode && S_ISBLK(inode->i_mode) && p->bdev) {
+		set_blocksize(p->bdev, p->old_block_size);
+		blkdev_put(p->bdev, FMODE_READ | FMODE_WRITE | FMODE_EXCL);
+	}
+	destroy_swap_extents(p);
+	swap_cgroup_swapoff(p->type);
+	spin_lock(&swap_lock);
+	p->swap_file = NULL;
+	p->flags = 0;
+	spin_unlock(&swap_lock);
+	vfree(swap_map);
+	if (swap_file) {
+		if (inode && S_ISREG(inode->i_mode)) {
+			mutex_unlock(&inode->i_mutex);
+			inode = NULL;
+		}
+		filp_close(swap_file, NULL);
+	}
+ out:
+	if (page && !IS_ERR(page)) {
+		kunmap(page);
+		page_cache_release(page);
+	}
+
+	if (inode && S_ISREG(inode->i_mode))
+		mutex_unlock(&inode->i_mutex);
+	return error;
+}
+
+EXPORT_SYMBOL(swapon);
+#endif /* CONFIG_ZRAM_FOR_ANDROID */
+
 SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)
 {
 	struct swap_info_struct *p;
diff --git a/mm/vmscan.c b/mm/vmscan.c
index 5326f98..a1bb327 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -178,6 +178,12 @@ static unsigned long zone_nr_lru_pages(struct zone *zone,
 	return zone_page_state(zone, NR_LRU_BASE + lru);
 }
 
+#ifdef CONFIG_ZRAM_FOR_ANDROID
+extern int swap_inactive_pagelist(unsigned int page_swap_cluster);
+static unsigned int timer_counter = 0;
+#define COMPCACHE_DELAY_COUNTER 500
+#define COMPCACHE_GOOD_TIME 10000
+#endif /* CONFIG_ZRAM_FOR_ANDROID */
 
 /*
  * Add a shrinker callback to be called from the vm
@@ -766,7 +772,10 @@ static noinline_for_stack void free_page_list(struct list_head *free_pages)
 /*
  * shrink_page_list() returns the number of reclaimed pages
  */
-static unsigned long shrink_page_list(struct list_head *page_list,
+#ifndef CONFIG_ZRAM_FOR_ANDROID
+static
+#endif /* CONFIG_ZRAM_FOR_ANDROID */
+unsigned long shrink_page_list(struct list_head *page_list,
 				      struct zone *zone,
 				      struct scan_control *sc)
 {
@@ -1268,7 +1277,10 @@ static unsigned long isolate_pages_global(unsigned long nr,
  * clear_active_flags() is a helper for shrink_active_list(), clearing
  * any active bits from the pages in the list.
  */
-static unsigned long clear_active_flags(struct list_head *page_list,
+#ifndef CONFIG_ZRAM_FOR_ANDROID
+static
+#endif /* CONFIG_ZRAM_FOR_ANDROID */
+unsigned long clear_active_flags(struct list_head *page_list,
 					unsigned int *count)
 {
 	int nr_active = 0;
@@ -1499,6 +1511,11 @@ static inline bool should_reclaim_stall(unsigned long nr_taken,
 	unsigned long nr_taken;
 	unsigned long nr_anon;
 	unsigned long nr_file;
+
+#ifdef CONFIG_ZRAM_FOR_ANDROID
+	struct timeval start, end;
+#endif /* CONFIG_ZRAM_FOR_ANDROID */
+
 	isolate_mode_t reclaim_mode = ISOLATE_INACTIVE;
 
 	while (unlikely(too_many_isolated(zone, file, sc))) {
@@ -1509,6 +1526,34 @@ static inline bool should_reclaim_stall(unsigned long nr_taken,
 			return SWAP_CLUSTER_MAX;
 	}
 
+#ifdef CONFIG_ZRAM_FOR_ANDROID
+	/*
+	  use optimized compcache to swap pages firstly
+	*/
+	if (timer_counter == 0) {
+		long long  swap_time = 0;
+		do_gettimeofday(&start);
+		nr_reclaimed = swap_inactive_pagelist(SWAP_CLUSTER_MAX);
+		do_gettimeofday(&end);
+		swap_time = (long long)
+				((end.tv_sec - start.tv_sec) * USEC_PER_SEC +
+				 (end.tv_usec - start.tv_usec));
+		/*
+		  if the used time of optimized compcache is too long,
+		  delay to use optimzed, use original one for sometime
+		*/
+		if (swap_time > COMPCACHE_GOOD_TIME)
+			timer_counter = COMPCACHE_DELAY_COUNTER;
+		if (nr_reclaimed >= SWAP_CLUSTER_MAX)
+			return nr_reclaimed;
+		/* if there is no background application can be swapped,
+		   use original one for sometime */
+		else
+			timer_counter = COMPCACHE_DELAY_COUNTER;
+	} else
+		timer_counter--;
+#endif /* CONFIG_ZRAM_FOR_ANDROID */
+
 	set_reclaim_mode(priority, sc, false);
 	if (sc->reclaim_mode & RECLAIM_MODE_LUMPYRECLAIM)
 		reclaim_mode |= ISOLATE_ACTIVE;
@@ -1574,6 +1619,44 @@ static inline bool should_reclaim_stall(unsigned long nr_taken,
 	return nr_reclaimed;
 }
 
+#ifdef CONFIG_ZRAM_FOR_ANDROID
+unsigned long
+zone_id_shrink_pagelist(struct zone *zone, struct list_head *page_list)
+{
+	unsigned long nr_reclaimed = 0;
+	unsigned long nr_anon;
+	unsigned long nr_file;
+
+	struct scan_control sc = {
+		.gfp_mask = GFP_USER,
+		.may_writepage = 1,
+		.nr_to_reclaim = SWAP_CLUSTER_MAX,
+		.may_unmap = 1,
+		.may_swap = 1,
+		.swappiness = vm_swappiness,
+		.order = 0,
+		.mem_cgroup = NULL,
+		.nodemask = NULL,
+	};
+
+	spin_lock_irq(&zone->lru_lock);
+
+	update_isolated_counts(zone, &sc, &nr_anon, &nr_file, page_list);
+
+	spin_unlock_irq(&zone->lru_lock);
+
+	nr_reclaimed = shrink_page_list(page_list, zone, &sc);
+
+	__count_zone_vm_events(PGSTEAL, zone, nr_reclaimed);
+
+	putback_lru_pages(zone, &sc, nr_anon, nr_file, page_list);
+
+	return nr_reclaimed;
+}
+
+EXPORT_SYMBOL(zone_id_shrink_pagelist);
+#endif /* CONFIG_ZRAM_FOR_ANDROID */
+
 /*
  * This moves pages from the active list to the inactive list.
  *
@@ -3055,7 +3138,11 @@ unsigned long shrink_all_memory(unsigned long nr_to_reclaim)
 	struct reclaim_state reclaim_state;
 	struct scan_control sc = {
 		.gfp_mask = GFP_HIGHUSER_MOVABLE,
+#if defined(CONFIG_SLP) && defined(CONFIG_FULL_PAGE_RECLAIM)
+		.may_swap = 0,
+#else
 		.may_swap = 1,
+#endif
 		.may_unmap = 1,
 		.may_writepage = 1,
 		.nr_to_reclaim = nr_to_reclaim,
-- 
1.7.10
